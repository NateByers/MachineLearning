---
title: 'B529: Homework 3'
author: "Nathan Byers"
date: "Wednesday, April 7, 2015"
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{B529 Machine Learning}
- \fancyfoot[CO,CE]{Source https://github.com/NateByers/MachineLearning/tree/master/Homework3}
- \fancyfoot[LE,RO]{\thepage}
output: pdf_document
---

# Question 1

In the clustering problems, the input data set contains the following six data points (1,1), (0,1), (1,0), (3,3) (4,4), (5,4). Manhattan distance is chosen to compute the distance between two data points. The k-means clustering method is used to find the clusters with k = 2 and initial centroids (3,3) and (5,4). Please describe the steps of the k-means clustering. (In each step, provide the information about cluster assignments of data points, centroids; and when the algorithm stops). (25 points)

# Answer 1

First I create the points in R and plot them.

```{r, fig.height=3, fig.width=3}
x <- c(1, 0, 1, 3, 4, 5)
y <- c(1, 1, 0, 3, 4, 4)
seed1 <- c(3, 3)
seed2 <- c(5, 4)

plot(x = x, y = y)
points(c(seed1[1], seed2[1]),
       c(seed1[2], seed2[2]),
       col = c("red", "blue"),
       pch = 19)
```

Then I create a data frame that gives the initial state of the points.

```{r}
group.df <- data.frame(x, y, group = NA,
                       centr_x = NA, centr_y = NA)
group.df[group.df$x == 3 & 
           group.df$y == 3, 
         c("group", "centr_x", "centr_y")] <- c(1, 3, 3)

group.df[group.df$x == 5 & 
           group.df$y == 4,
         c("group", "centr_x", "centr_y")] <- c(2, 5, 4)

manDist <- function(x, y, centr_x, centr_y){
  abs(centr_x - x) + 
    abs(centr_y - y)
}

group.df$dist1 <- unlist(mapply(manDist, group.df$x,
                       group.df$y,
                       MoreArgs = list(
                         centr_x = seed1[1],
                         centr_y = seed1[2])))

group.df$dist2 <- unlist(mapply(manDist, group.df$x,
                                group.df$y,
                                MoreArgs = list(
                                  centr_x = seed2[1],
                                  centr_y = seed2[2])))

group.df$group <- apply(group.df, 1, function(row){
  if(row["dist1"] == 
       min(row[c("dist1", "dist2")])){
    1
  }else{2}
})

group.df$change <- NA

group.df
```

Now we find the new centroids, recalculate the distances from each point to
the new centroids, and regroup the points according to the shortest Manhattan 
distance to the new centroids.

```{r}
reGroup <- function(df){
  center_x1 <- mean(df[df$group == 1, "x"])
  center_y1 <- mean(df[df$group == 1, "y"])
  center_x2 <- mean(df[df$group == 2, "x"])
  center_y2 <- mean(df[df$group == 2, "y"])
  df[df$group == 1, "centr_x"] <- center_x1
  df[df$group == 1, "centr_y"] <- center_y1
  df[df$group == 2, "centr_x"] <- center_x2
  df[df$group == 2, "centr_y"] <- center_y2
  df$dist1 <- unlist(mapply(manDist, df$x,
                            df$y,
                            MoreArgs = list(
                              centr_x = center_x1,
                              centr_y = center_y1)))
  df$dist2 <- unlist(mapply(manDist, df$x,
                            df$y,
                            MoreArgs = list(
                              centr_x = center_x2,
                              centr_y = center_y2)))
  g.initial <- df$group
  g <- apply(df, 1, function(row){
    if(row["dist1"] != row["dist2"]){
      if(row["dist1"] == 
           min(row[c("dist1", "dist2")])){
        1
      }else{2}
    }else{0}
  })
  df$group <- g
  df$change <- sapply(1:length(g), function(i){
    !(g[i] == g.initial[i])
  })
  df
}

group.df2 <- reGroup(group.df)

group.df2
```

We can see that at least one point changed. Below is a plot of the new groups.

```{r, fig.height=3, fig.width=3}
plotGroups <- function(df){
  group.cols <- df$group
  group.cols[group.cols == 1] <- "red"
  group.cols[group.cols == 2] <- "blue"
  group.cols[group.cols == 0] <- "black"
  plot(df$x, df$y, pch = 19,
       col = group.cols)
}

plotGroups(group.df2)

```

Now we regroup again, because at least one point changed groups in the last
iteration.

```{r, fig.height=3, fig.width=3}
group.df3 <- reGroup(group.df2)

plotGroups(group.df3)

group.df3

```

We can see in the table that no points have changed groups, and the plot looks the
same as the plot in the last iteration. So we stop here, because the algorithm has
converged.


# Question 2

Suppose DNA bases in a protein-coding region follow the distribution: 

DNA base   Probability
---------  ------------
A          $\theta $
C          $\frac{1}{4} $
G          $\frac{1}{2} $
T          $\frac{1}{4} - \theta $

In an experiment, the number of observed “A” or “C” bases at the position is x, and the number of observed “G” or “T” bases at the position is y. The EM algorithm can be used to find parameter $\theta $. Describe the Expectation step and Maximization step in the EM algorithm (25 points)

# Answer 2

If the number of A, C, G, and T bases are a, c, g, and t respectively, then the likelihood is

$$P(a, c, g, t | \theta) = C(\theta)^{a}\left(\frac{1}{4}\right)^{c}\left(\frac{1}{2}\right)^{g}\left(\frac{1}{4}-\theta\right)^{t}$$

We take the log, take the derivative with respect to $\theta$, and set it equal to 0 to solve for $\theta$.

$$\ln P(a, c, g, t | \theta) = \ln C + a\ln\theta + c\ln\frac{1}{4} + g\ln\frac{1}{2} + t\ln\left(\frac{1}{4}-\theta\right)$$

$$\frac{\partial }{\partial \theta }\ln P(a, c, g, t | \theta) = \frac{a}{\theta} - \frac{t}{\frac{1}{4}-\theta}=0$$

$$\theta = \frac{a}{4(t+a)}$$

We know $\frac{a}{c}=\frac{\theta}{\frac{1}{4}}$ and $a + c = x$, so $a=\frac{c\theta}{\frac{1}{4}}=\frac{(x-a)\theta}{\frac{1}{4}}=\frac{\theta}{\frac{1}{4}+\theta}x$. We also know $\frac{t}{g}=\frac{\frac{1}{4}-\theta}{\frac{1}{2}} $ and $g+t=y$, so $t=\frac{(\frac{1}{4}-\theta)g}{\frac{1}{2}}=\frac{(\frac{1}{4}-\theta)(y-t)}{\frac{1}{2}}=\frac{\frac{1}{4}-\theta}{\frac{3}{4}-\theta}y $. For the algorithm, we give an initial value to $\theta$, and if we know the value of x and y, we solve for a and t. Using these initial values we solve for a new $\theta$,

$$\theta_{new} = \frac{a}{4(t+a)}$$

We then use $\theta_{new}$ to find $a_{new}$ and $t_{new}$. The steps are repeated until convergence.

Suppose x = 12,000 and y = 13,000 and the initial $\theta$ is 0.1. Below is the EM algorithm.

```{r}
theta <- 0.1
calcA <- function(theta, x = 12000){
  (x * theta)/((1/4) + theta)
}
calcT <- function(theta, y = 13000){
  (((1/4) - theta) * y)/((3/4)-theta)
}
calcTheta <- function(a, t){
  a/(4 * ( t+ a))
}
  
em.df <- data.frame(n = 0, a = NA, t = NA, theta = theta)
theta.diff <- theta
while(theta.diff> 0.00001){
  n.last = em.df[nrow(em.df), "n"]
  theta.old = em.df[em.df$n == n.last, "theta"]
  a.new = calcA(theta.old)
  t.new = calcT(theta.old)
  theta.new = calcTheta(a.new, t.new)
  n.new = n.last + 1
  em.new.df = data.frame(n = n.new, a = a.new,
                          t = t.new, theta = theta.new)
  em.df <<- rbind(em.df, em.new.df) 
  theta.diff <<- abs(theta.new - theta.old)
  if(n.new > 1000) break
}

```

The algorithm converged after 67 iterations

```{r}
head(em.df)
tail(em.df)
```


# Question 3

Use the ID3 method to construct a decision tree using the following data set for credit card application. (25 points)

Age        Income  Gender  Risk
------     ------- ------- -----
<25        >50K    M       High
<25        >50K    F       High
$\geq 25$  <50K    F       High
$\geq 25$  >50K    F       Low
$\geq 25$  >50K    M       Low
<25        <50K    M       High

# Answer 3

First, I put the factors in a data frame and write a function for finding the 
entropy of a factor based on how it splits the risk column.

```{r, message=FALSE}
age <- factor(c(0, 0, 1, 1, 1, 0), labels = c("<25", ">25")) # 0 = <25, 1 = >25
income <- factor(c(0, 0, 1, 0, 0, 1), labels = c(">50k", "<50k")) # 0 = >50K, 1 = <50K
gender <- factor(c(0, 1, 1, 1, 0, 0), labels = c("M", "F")) # 0 = M, 1 = F
risk <- factor(c(0, 0, 0, 1, 1, 0), labels = c("High", "Low")) # 0 = High, 1 = Low

df <- data.frame(age, income, gender, risk)

df

library(dplyr)

calcEntropy <- function(split.factor, outcome){
  # get proportions
  proportions = sapply(split(split.factor, split.factor),
                       function(level, total){length(level)/total},
                       total = length(split.factor))
  if(identical(split.factor, outcome)){
    sum(sapply(proportions, function(x) -x * log(x, 2)))
  }else{
  # put in data frame, group by factor and outcome,
  # get total counts
  df = data.frame(split.factor, outcome, I = 1)
  df = group_by(df, split.factor, outcome)
  df.sum = summarize(df, count = sum(I))
  # get the levels of the split.factor and subset
  split.levels = levels(split.factor)
  df.split1 = filter(df.sum, split.factor == split.levels[1])
  # entropy of outcome after split, level 1
  entropy1 = sapply(df.split1$count, function(x, total) {x/total},
                    total = sum(df.split1$count))
  entropy1 = sum(sapply(entropy1, function(x) -x * log(x, 2)))
  # entropy of outcome after split, level 2
  df.split2 = filter(df.sum, split.factor == split.levels[2])
  entropy2 = sapply(df.split2$count, function(x, total) {x/total},
                    total = sum(df.split2$count))
  entropy2 = sum(sapply(entropy2, function(x) -x * log(x, 2)))
  # sum entropies proportionally
  proportions[[1]] * entropy1 + proportions[[2]] * entropy2
  }
}

```

If we calculate the entropy using the Gender column, this is the output of the function.

```{r}
calcEntropy(gender, risk)
```

I loop through all of the columns of the data frame, including the Risk column to get the initial entropy, and I get the change in entropy for each factor.

```{r}
entropy.list <- lapply(df, calcEntropy, outcome = df$risk)

information.gain <- entropy.list[[4]] - unlist(entropy.list[1:3]) 

information.gain

```
The result indicates that Age is the first factor to split on.

Next we split the table using Age.

```{r}
df.age.split <- lapply(levels(age), 
                       function(i) df[df$age == i, 
                                      c("income", "gender", "risk")])

df.age.split

```
The first data frame in the `df.age.split` object has 0 entropy as a starting point, so there won't be any information gain. So we find the entropy when we split the second data frame using the remaining factors.

```{r}
df2 <- df.age.split[[2]]

entropy.list2 <- lapply(df2, calcEntropy, 
                        outcome = df2$risk)

information.gain2 <- entropy.list[[3]] - unlist(entropy.list2[1:2])

information.gain2
```

We can see that Income has the highest information gain, so we split on that factor next.

Now we split the data by Income and we can see that there will be no information gained from the Gender factor.

```{r}
df.income.split <- lapply(levels(income),
                          function(i) df2[df2$income == i,
                                          c("gender", "risk")])
df.income.split
```

So we stop here.
                                                    

# Question 4

A linear SVM is used to analyze a 2-dimensional data set, and the solution $\alpha $ to the quadratic programming program contains only three non-zero elements $\alpha _{1} = 0.05 $, $\alpha _{2} = 0.05 $, and $\alpha_{3} = 0.1 $. The corresponding input data points are $\mathbf{x}_{1} = [0,3]^{T} $, $y_{1} = 1 $, $\mathbf{x}_{2} = [2,4]^{T} $, $y_{2} = 1$, $\mathbf{x}_{3} = [3,-0.5]^{T} $, $y_{3} = -1 $. Compute the weight vector **w** and parameter b for the separating line with the maximum margin (15 points). Compute the margin of the separating line (10 points). 

# Answer 4

$$\mathbf{w} = \sum \alpha y \boldsymbol{x} =0.05 \cdot 1 \cdot \begin{bmatrix}
0\\ 
3
\end{bmatrix}
+ 0.05 \cdot1 \cdot \begin{bmatrix}
2\\
4
\end{bmatrix}
0.1 \cdot -1 \cdot \begin{bmatrix} 
3\\
-0.5
\end{bmatrix} = \begin{bmatrix} 
-0.2\\
0.4
\end{bmatrix}$$

$$y_{n}(\mathbf{w}^{\mathbf{T}}\mathbf{x}_{n}+b)=1$$

$$1(\begin{bmatrix}
-0.2 & 0.4
\end{bmatrix} \begin{bmatrix} 
0\\
3
\end{bmatrix}+b)=1$$

$$b = -0.2 $$

$$margin = \frac{1}{\left \| \mathbf{w} \right \|}=\sqrt{-0.2^{2}+0.4^2}=0.447$$
