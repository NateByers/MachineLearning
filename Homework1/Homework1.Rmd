---
title: 'B529: Homework 1'
author: "Nathan Byers"
date: "Tuesday, February 03, 2015"
output: pdf_document
---

# Question 1
*Please use the perceptron algorithm to manually find a weight vector* **w** = [w0,w1,w2]^T^  *for linear classification of 3 data points* (**x**<sub>i</sub>,y<sub>i) *satisfying  sign* (**w**^T^**x**<sub>i</sub>) = y<sub>i</sub>. *The input data is* 

**x**<sub>1</sub>=(1,2,2)^T^  y<sub>1</sub>=+1

**x**<sub>2</sub>=(1,1,2)^T^  y<sub>2</sub>=+1

**x**<sub>3</sub>=(1,2,0)^T^  y<sub>3</sub>=-1

*The first element in vector* **x** is always x<sub>0</sub>=1 *(See slides in lecture 1).*

*In the first step of the perceptron algorithm, the initial assignment of* **w** is [0,6,6]^T^. *Please provide the intermediate steps of the computation. (30 points)*

# Answer 1

* Step 1: **w** = [0,6,6]^T^
* Step 2: 

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(24)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(18)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(12)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
0\\
6\\ 
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-1\\ 
4\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(19)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(15)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(7)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
-1\\
4\\
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-2\\ 
2\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(14)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(12)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(2)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
-2\\ 
2\\ 
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-4\\ 
0\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(8)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(8)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(-4) = -1$$

Final **w** = (-4, 0, 6)^T^

# Question 2

*Please use R to manually compute the weight vector* **w** = [w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>]^T^ *for linear regression of N input data points* (x<sub>i</sub>,y<sub>i</sub>) *that minimizes the sum of squared error* (**w**^T^**x**<sub>i</sub> – y<sub>i</sub>)^2^. *The input data is* 

**x**<sub>1</sub>=(1,2,3)^T^    y<sub>1</sub> =  2
     
**x**<sub>2</sub>=(1,1,2)^T^    y<sub>2</sub> =  1

**x**<sub>3</sub>=(1,1,0)^T^    y<sub>3</sub> = -1

**x**<sub>4</sub>=(1,-1,-2)^T^  y<sub>4</sub>= -2

*The first element in vector* **x** *is always* x<sub>0</sub>=1 *(See slides in lecture 1)*

*Please provide the intermediate steps of the computation (20 points).*

# Answer 2

```{r}
X <- matrix(c(1, 2, 3, 1, 1, 2, 1, 1, 0, 1, -1, -2), nrow = 4, byrow = TRUE)
X
y <- matrix(c(2, 1, -1, -2), ncol = 1)
y
Xt <- t(X)
Xt
w <- solve(Xt %*% X) %*% Xt %*% y
w
```

# Question 3

*In Fisher’s linear discriminant analysis, we search for a vector* **w** *such that all data points are well separately after they are projected to the direction defined by* **w**. *When the input data points are* 

x1=(3,4)^T^    y1 =  1

x2=(1,2)^T^    y2 =  1

x3=(1,0)^T^    y3 = -1

x4=(1,-2)^T^  y4=  -1

*(Note that the first element in vector* **x** *is NOT x0=1 in this problem), please use R to manually find a vector* **w**=[w1,w2] *with Fisher’s LDA and provide the intermediate steps (20 points).*

# Answer 3
Here I create a matrix with each column containing an **x** vector as well as the y scalar.
```{r}
Xy <- matrix(c(3, 4, 1, 1, 2, 1, 1, 0, -1, 1, -2, -1), ncol = 4)
colnames(Xy) <- c("x1hat", "x2hat", "x3hat", "x4hat")
rownames(Xy) <- c("x1", "x2", "y")
Xy
```
Now I calculate the **m** vectors.
```{r}
m1hat <- matrix(apply(Xy[, Xy[3, ] == 1], 1, mean)[-3], ncol = 1)
rownames(m1hat) <- c("m1", "m2")
m1hat
m2hat <- matrix(apply(Xy[, Xy[3, ] == -1], 1, mean)[-3], ncol = 1)
rownames(m2hat) <- c("m1", "m2")
m2hat
```
Then I create a function that takes the difference between an **x** vector and 
the appropriate **m** vector, and calculates the outer product between that vector
and its transpose.
```{r}
outerProduct <- function(i, Xy.matrix, m1, m2){
  if(Xy.matrix[3, i] == 1){m <- m1} else {m <- m2}
  v <- Xy.matrix[1:2, i] - m
  v %*% t(v)
}
```
Now I loop through the columns of the **Xy** matrix using the `outerProduct()` function, 
and I get a matrix whose rows can be summed to find the four values of the **Sw** matrix.

```{r}
products <- sapply(1:dim(Xy)[2], outerProduct, Xy.matrix = Xy,
                         m1 = m1hat, m2 = m2hat)
products

Sw <- matrix(rowSums(products), ncol = 2)
Sw
```

Now I find the optimal solution, $\mathbf{w}\propto \mathbf{S}_{w}^{-1}(\mathbf{m}_{1}-\mathbf{m}_{2})$.

```{r}
w <- solve(Sw) %*% (m1hat - m2hat)
w
```

# Question 4

*In the problem of determining if a peptide-spectrum-match is a correct one or not, we have the following problem:*

* *Input: scores of peptide spectrum matches:* $\mathbf{x}_{i} = [x]$
* *Output: if the spectrum is generated from the peptide:* $\mathbf{y}_{i} = +1/-1$
* *Prior probity:* *P*(*y* = 1)  = 0.1, *P*(*y* = -1) = 0.9
* *Liklihood:*

$$P(x|y=1)=\frac{1}{2\sqrt{2\pi }}e\frac{-(x-10)^{2}}{8}$$

$$P(x|y=-1)=\frac{1}{\sqrt{2\pi }}e\frac{-(x-5)^{2}}{2}$$ 

* *Cost function:*

*e*(*f*(**x**), *g*(**x**))  | +1  | -1
---------------------------- | --- | ----
+1                           | 0   | 1000
-1                           | 1   | 0

*Please compute the classification function that minimizes the cost (error). Please provide the intermediate steps of the computation. (30 points)*

# Answer 4

$$ln\lambda =ln\frac{P(y=-1)}{P(y=1)}\frac{c_{1,-1}}{c_{-1,1}}=ln\frac{0.9}{0.1}\frac{1000}{1}=ln(9.10498)$$

$$ln\mathit{l}(x) =ln\frac{P(\mathbf{x}|y=-1)}{P(\mathbf{x}|y=1)}=ln\frac{1}{2\sqrt{2\pi }}e\frac{-(x-10)^{2}}{8}-ln\frac{1}{\sqrt{2\pi }}e\frac{-(x-5)^{2}}{2}$$
