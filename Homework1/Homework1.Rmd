---
title: 'B529: Homework 1'
author: "Nathan Byers"
date: "Tuesday, February 03, 2015"
output: pdf_document
---

# Question 1
*Please use the perceptron algorithm to manually find a weight vector* **w** = [w0,w1,w2]^T^  *for linear classification of 3 data points* (**x**<sub>i</sub>,y<sub>i) *satisfying  sign* (**w**^T^**x**<sub>i</sub>) = y<sub>i</sub>. *The input data is* 

**x**<sub>1</sub>=(1,2,2)^T^  y<sub>1</sub>=+1

**x**<sub>2</sub>=(1,1,2)^T^  y<sub>2</sub>=+1

**x**<sub>3</sub>=(1,2,0)^T^  y<sub>3</sub>=-1

*The first element in vector* **x** is always x<sub>0</sub>=1 *(See slides in lecture 1).*

*In the first step of the perceptron algorithm, the initial assignment of* **w** is [0,6,6]^T^. *Please provide the intermediate steps of the computation. (30 points)*

# Answer 1

* Step 1: **w** = [0,6,6]^T^
* Step 2: 

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(24)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(18)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
0 & 6 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(12)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
0\\
6\\ 
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-1\\ 
4\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(19)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(15)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-1 & 4 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(7)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
-1\\
4\\
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-2\\ 
2\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(14)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(12)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-2 & 2 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(2)\neq -1$$
$$\boldsymbol{w}=\boldsymbol{w}+y_{3}\boldsymbol{x}_{3}=\begin{bmatrix}
-2\\ 
2\\ 
6
\end{bmatrix} + -1 \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix} = \begin{bmatrix}
-4\\ 
0\\ 
6
\end{bmatrix}$$

* Step 2: iteration

$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
2
\end{bmatrix}\right )=sign(8)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
1\\ 
2
\end{bmatrix}\right )=sign(8)=1$$
$$sign(\boldsymbol{w}^{T}\boldsymbol{x}_{1})= sign\left ( \begin{bmatrix}
-4 & 0 & 6
\end{bmatrix} \cdot \begin{bmatrix}
1\\ 
2\\ 
0
\end{bmatrix}\right )=sign(-4) = -1$$

Final **w** = (-4, 0, 6)^T^

# Question 2

*Please use R to manually compute the weight vector* **w** = [w<sub>0</sub>, w<sub>1</sub>, w<sub>2</sub>]^T^ *for linear regression of N input data points* (x<sub>i</sub>,y<sub>i</sub>) *that minimizes the sum of squared error* (**w**^T^**x**<sub>i</sub> – y<sub>i</sub>)^2^. *The input data is* 

**x**<sub>1</sub>=(1,2,3)^T^    y<sub>1</sub> =  2
     
**x**<sub>2</sub>=(1,1,2)^T^    y<sub>2</sub> =  1

**x**<sub>3</sub>=(1,1,0)^T^    y<sub>3</sub> = -1

**x**<sub>4</sub>=(1,-1,-2)^T^  y<sub>4</sub>= -2

*The first element in vector* **x** *is always* x<sub>0</sub>=1 *(See slides in lecture 1)*

*Please provide the intermediate steps of the computation (20 points).*

# Answer 2

```{r}
X <- matrix(c(1, 2, 3, 1, 1, 2, 1, 1, 0, 1, -1, -2), nrow = 4, byrow = TRUE)
X
y <- matrix(c(2, 1, -1, -2), ncol = 1)
y
Xt <- t(X)
Xt
w <- solve(Xt %*% X) %*% Xt %*% y
w
```

# Question 3

*In Fisher’s linear discriminant analysis, we search for a vector* **w** *such that all data points are well separately after they are projected to the direction defined by* **w**. *When the input data points are* 

x1=(3,4)^T^    y1 =  1

x2=(1,2)^T^    y2 =  1

x3=(1,0)^T^    y3 = -1

x4=(1,-2)^T^  y4=  -1

*(Note that the first element in vector* **x** *is NOT x0=1 in this problem), please use R to manually find a vector* **w**=[w1,w2] *with Fisher’s LDA and provide the intermediate steps (20 points).*

# Answer 3
Here I create a matrix with each column containing an **x** vector as well as the y scalar.
```{r}
Xy <- matrix(c(3, 4, 1, 1, 2, 1, 1, 0, -1, 1, -2, -1), ncol = 4)
colnames(Xy) <- c("x1hat", "x2hat", "x3hat", "x4hat")
rownames(Xy) <- c("x1", "x2", "y")
Xy
```
Now I calculate the **m** vectors.
```{r}
m1hat <- matrix(apply(Xy[, Xy[3, ] == 1], 1, mean)[-3], ncol = 1)
rownames(m1hat) <- c("m1", "m2")
m1hat
m2hat <- matrix(apply(Xy[, Xy[3, ] == -1], 1, mean)[-3], ncol = 1)
rownames(m2hat) <- c("m1", "m2")
m2hat
```
